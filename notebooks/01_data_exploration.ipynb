{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bf4e3c",
   "metadata": {},
   "source": [
    "# AutoTune ML Trainer - Data Exploration\n",
    "\n",
    "**Created by Sergie Code - AI Tools for Musicians** üéµ\n",
    "\n",
    "This notebook provides a comprehensive introduction to audio data exploration and pitch detection for training intelligent AutoTune models.\n",
    "\n",
    "## Learning Objectives\n",
    "- Set up the environment and validate dependencies\n",
    "- Load and explore audio data using Librosa\n",
    "- Implement pitch detection with CREPE and Librosa\n",
    "- Visualize audio signals and pitch information\n",
    "- Prepare data for neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002da69",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's import all necessary libraries and validate our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae7090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "# Machine learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "# Pitch detection\n",
    "try:\n",
    "    import crepe\n",
    "    CREPE_AVAILABLE = True\n",
    "    print(\"‚úÖ CREPE available for advanced pitch detection\")\n",
    "except ImportError:\n",
    "    CREPE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è CREPE not available. Install with: pip install crepe\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"üéµ AutoTune ML Trainer Environment\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Librosa version: {librosa.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ffbd6",
   "metadata": {},
   "source": [
    "## 2. Audio Data Loading and Preprocessing\n",
    "\n",
    "Let's start by generating some test audio and learning how to load and process audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test audio signals\n",
    "def generate_test_audio(frequency=440, duration=2.0, sample_rate=44100):\n",
    "    \"\"\"Generate a simple sine wave for testing.\"\"\"\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "    audio = np.sin(2 * np.pi * frequency * t)\n",
    "    return audio, sample_rate\n",
    "\n",
    "# Create test signals\n",
    "test_frequencies = [220, 440, 880]  # A3, A4, A5\n",
    "test_signals = {}\n",
    "\n",
    "for freq in test_frequencies:\n",
    "    audio, sr = generate_test_audio(freq, duration=1.0)\n",
    "    test_signals[f\"A{freq}Hz\"] = (audio, sr)\n",
    "    \n",
    "print(\"Generated test audio signals:\")\n",
    "for name, (audio, sr) in test_signals.items():\n",
    "    print(f\"  {name}: {len(audio)} samples, {sr} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test audio signals\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "fig.suptitle('Test Audio Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, (name, (audio, sr)) in enumerate(test_signals.items()):\n",
    "    # Time domain\n",
    "    time = np.linspace(0, len(audio)/sr, len(audio))\n",
    "    axes[i, 0].plot(time[:1000], audio[:1000])  # First 1000 samples\n",
    "    axes[i, 0].set_title(f'{name} - Time Domain')\n",
    "    axes[i, 0].set_xlabel('Time (s)')\n",
    "    axes[i, 0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Frequency domain\n",
    "    fft = np.fft.fft(audio[:4096])  # Use first 4096 samples\n",
    "    freqs = np.fft.fftfreq(4096, 1/sr)\n",
    "    magnitude = np.abs(fft)\n",
    "    \n",
    "    axes[i, 1].plot(freqs[:2048], magnitude[:2048])\n",
    "    axes[i, 1].set_title(f'{name} - Frequency Domain')\n",
    "    axes[i, 1].set_xlabel('Frequency (Hz)')\n",
    "    axes[i, 1].set_ylabel('Magnitude')\n",
    "    axes[i, 1].set_xlim(0, 2000)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd13b84",
   "metadata": {},
   "source": [
    "## 3. Pitch Detection with CREPE and Librosa\n",
    "\n",
    "Now let's compare different pitch detection methods and analyze their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a97228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_pitch_librosa(audio, sr):\n",
    "    \"\"\"Detect pitch using Librosa's piptrack.\"\"\"\n",
    "    pitches, magnitudes = librosa.piptrack(y=audio, sr=sr, fmin=80, fmax=2000)\n",
    "    \n",
    "    # Extract fundamental frequency\n",
    "    pitch_values = []\n",
    "    for t in range(pitches.shape[1]):\n",
    "        index = magnitudes[:, t].argmax()\n",
    "        pitch = pitches[index, t]\n",
    "        if pitch > 0:\n",
    "            pitch_values.append(pitch)\n",
    "    \n",
    "    return np.array(pitch_values)\n",
    "\n",
    "def detect_pitch_crepe(audio, sr):\n",
    "    \"\"\"Detect pitch using CREPE (if available).\"\"\"\n",
    "    if not CREPE_AVAILABLE:\n",
    "        return None, None\n",
    "    \n",
    "    # Resample to 16kHz for CREPE\n",
    "    if sr != 16000:\n",
    "        audio_16k = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "    else:\n",
    "        audio_16k = audio\n",
    "    \n",
    "    time, frequency, confidence, _ = crepe.predict(\n",
    "        audio_16k, sr=16000, model_capacity='medium', step_size=10\n",
    "    )\n",
    "    \n",
    "    return frequency, confidence\n",
    "\n",
    "# Test pitch detection on our test signals\n",
    "pitch_results = {}\n",
    "\n",
    "for name, (audio, sr) in test_signals.items():\n",
    "    true_freq = float(name.replace('A', '').replace('Hz', ''))\n",
    "    \n",
    "    # Librosa pitch detection\n",
    "    librosa_pitches = detect_pitch_librosa(audio, sr)\n",
    "    librosa_mean = np.mean(librosa_pitches) if len(librosa_pitches) > 0 else 0\n",
    "    \n",
    "    # CREPE pitch detection\n",
    "    if CREPE_AVAILABLE:\n",
    "        crepe_freq, crepe_conf = detect_pitch_crepe(audio, sr)\n",
    "        # Filter by confidence\n",
    "        valid_crepe = crepe_freq[crepe_conf > 0.8]\n",
    "        crepe_mean = np.mean(valid_crepe) if len(valid_crepe) > 0 else 0\n",
    "    else:\n",
    "        crepe_mean = 0\n",
    "    \n",
    "    pitch_results[name] = {\n",
    "        'true_frequency': true_freq,\n",
    "        'librosa_detected': librosa_mean,\n",
    "        'crepe_detected': crepe_mean,\n",
    "        'librosa_error': abs(librosa_mean - true_freq),\n",
    "        'crepe_error': abs(crepe_mean - true_freq) if crepe_mean > 0 else float('inf')\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(pitch_results).T\n",
    "print(\"Pitch Detection Comparison:\")\n",
    "print(results_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66173475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pitch detection accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "methods = ['Librosa', 'CREPE']\n",
    "frequencies = [220, 440, 880]\n",
    "librosa_errors = [results_df.loc[f'A{f}Hz', 'librosa_error'] for f in frequencies]\n",
    "crepe_errors = [results_df.loc[f'A{f}Hz', 'crepe_error'] for f in frequencies]\n",
    "\n",
    "x = np.arange(len(frequencies))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, librosa_errors, width, label='Librosa', alpha=0.8)\n",
    "if CREPE_AVAILABLE:\n",
    "    ax1.bar(x + width/2, crepe_errors, width, label='CREPE', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Test Frequency (Hz)')\n",
    "ax1.set_ylabel('Absolute Error (Hz)')\n",
    "ax1.set_title('Pitch Detection Accuracy Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(frequencies)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Detection confidence over time (using 440Hz signal)\n",
    "if CREPE_AVAILABLE:\n",
    "    audio_440, sr_440 = test_signals['A440Hz']\n",
    "    crepe_freq, crepe_conf = detect_pitch_crepe(audio_440, sr_440)\n",
    "    \n",
    "    time_axis = np.linspace(0, len(audio_440)/sr_440, len(crepe_freq))\n",
    "    ax2.plot(time_axis, crepe_freq, label='Detected Frequency', linewidth=2)\n",
    "    ax2.axhline(y=440, color='red', linestyle='--', label='True Frequency (440 Hz)')\n",
    "    ax2.fill_between(time_axis, 0, crepe_freq, alpha=crepe_conf, label='Confidence')\n",
    "    \n",
    "    ax2.set_xlabel('Time (s)')\n",
    "    ax2.set_ylabel('Frequency (Hz)')\n",
    "    ax2.set_title('CREPE Pitch Detection Over Time')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'CREPE not available\\nInstall with: pip install crepe', \n",
    "             ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "    ax2.set_title('CREPE Pitch Detection (Not Available)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cb3263",
   "metadata": {},
   "source": [
    "## 4. Dataset Creation Pipeline\n",
    "\n",
    "Let's create a pipeline for preparing training data from audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ff4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project source to path\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom modules\n",
    "try:\n",
    "    from data.dataset_creator import DatasetCreator\n",
    "    from data.audio_preprocessor import AudioPreprocessor\n",
    "    from data.pitch_extractor import PitchExtractor\n",
    "    print(\"‚úÖ Successfully imported custom modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not import custom modules: {e}\")\n",
    "    print(\"Using fallback implementations...\")\n",
    "\n",
    "# Create sample dataset structure\n",
    "def create_sample_dataset():\n",
    "    \"\"\"Create a small sample dataset for demonstration.\"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    sample_dir = Path('../data/sample')\n",
    "    sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate diverse test audio\n",
    "    sample_rate = 44100\n",
    "    duration = 3.0\n",
    "    \n",
    "    samples = {\n",
    "        'vocal_low': generate_vibrato_audio(220, duration, sample_rate),\n",
    "        'vocal_mid': generate_vibrato_audio(440, duration, sample_rate),\n",
    "        'vocal_high': generate_vibrato_audio(880, duration, sample_rate),\n",
    "        'guitar_chord': generate_chord_audio([220, 277, 330], duration, sample_rate),\n",
    "        'noisy_vocal': add_noise_to_audio(*generate_vibrato_audio(440, duration, sample_rate))\n",
    "    }\n",
    "    \n",
    "    # Save audio files\n",
    "    for name, (audio, sr) in samples.items():\n",
    "        filepath = sample_dir / f\"{name}.wav\"\n",
    "        sf.write(filepath, audio, sr)\n",
    "        print(f\"Created: {filepath}\")\n",
    "    \n",
    "    return sample_dir\n",
    "\n",
    "def generate_vibrato_audio(base_freq, duration, sample_rate, vibrato_rate=5, vibrato_depth=0.02):\n",
    "    \"\"\"Generate audio with vibrato (frequency modulation).\"\"\"\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "    \n",
    "    # Apply vibrato\n",
    "    vibrato = vibrato_depth * np.sin(2 * np.pi * vibrato_rate * t)\n",
    "    freq_modulated = base_freq * (1 + vibrato)\n",
    "    \n",
    "    # Generate audio with amplitude envelope\n",
    "    audio = np.sin(2 * np.pi * freq_modulated * t)\n",
    "    envelope = np.exp(-t * 0.5)  # Decay envelope\n",
    "    audio *= envelope\n",
    "    \n",
    "    return audio, sample_rate\n",
    "\n",
    "def generate_chord_audio(frequencies, duration, sample_rate):\n",
    "    \"\"\"Generate audio with multiple frequencies (chord).\"\"\"\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "    audio = np.zeros_like(t)\n",
    "    \n",
    "    for freq in frequencies:\n",
    "        audio += np.sin(2 * np.pi * freq * t) / len(frequencies)\n",
    "    \n",
    "    return audio, sample_rate\n",
    "\n",
    "def add_noise_to_audio(audio, sample_rate, noise_level=0.05):\n",
    "    \"\"\"Add noise to audio signal.\"\"\"\n",
    "    noise = np.random.normal(0, noise_level, len(audio))\n",
    "    return audio + noise, sample_rate\n",
    "\n",
    "# Create the sample dataset\n",
    "sample_dataset_path = create_sample_dataset()\n",
    "print(f\"\\nSample dataset created at: {sample_dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71456d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the sample dataset\n",
    "audio_files = list(sample_dataset_path.glob('*.wav'))\n",
    "print(f\"Found {len(audio_files)} audio files in sample dataset:\")\n",
    "\n",
    "# Load and analyze each file\n",
    "dataset_analysis = {}\n",
    "\n",
    "for audio_file in audio_files:\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_file, sr=44100)\n",
    "    \n",
    "    # Basic statistics\n",
    "    duration = len(audio) / sr\n",
    "    rms_energy = np.sqrt(np.mean(audio**2))\n",
    "    max_amplitude = np.max(np.abs(audio))\n",
    "    \n",
    "    # Pitch detection\n",
    "    pitch_values = detect_pitch_librosa(audio, sr)\n",
    "    mean_pitch = np.mean(pitch_values) if len(pitch_values) > 0 else 0\n",
    "    \n",
    "    # Spectral features\n",
    "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sr))\n",
    "    zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(audio))\n",
    "    \n",
    "    dataset_analysis[audio_file.stem] = {\n",
    "        'duration': duration,\n",
    "        'rms_energy': rms_energy,\n",
    "        'max_amplitude': max_amplitude,\n",
    "        'mean_pitch': mean_pitch,\n",
    "        'spectral_centroid': spectral_centroid,\n",
    "        'zero_crossing_rate': zero_crossing_rate\n",
    "    }\n",
    "\n",
    "# Create analysis DataFrame\n",
    "analysis_df = pd.DataFrame(dataset_analysis).T\n",
    "print(\"\\nDataset Analysis:\")\n",
    "print(analysis_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a530cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset characteristics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Sample Dataset Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: RMS Energy distribution\n",
    "axes[0, 0].bar(analysis_df.index, analysis_df['rms_energy'])\n",
    "axes[0, 0].set_title('RMS Energy by Sample')\n",
    "axes[0, 0].set_ylabel('RMS Energy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Pitch distribution\n",
    "axes[0, 1].bar(analysis_df.index, analysis_df['mean_pitch'])\n",
    "axes[0, 1].set_title('Mean Pitch by Sample')\n",
    "axes[0, 1].set_ylabel('Frequency (Hz)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Spectral centroid\n",
    "axes[0, 2].bar(analysis_df.index, analysis_df['spectral_centroid'])\n",
    "axes[0, 2].set_title('Spectral Centroid by Sample')\n",
    "axes[0, 2].set_ylabel('Frequency (Hz)')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 4-6: Waveforms of first three samples\n",
    "for i, audio_file in enumerate(audio_files[:3]):\n",
    "    audio, sr = librosa.load(audio_file, sr=44100)\n",
    "    time = np.linspace(0, len(audio)/sr, len(audio))\n",
    "    \n",
    "    axes[1, i].plot(time, audio, alpha=0.7)\n",
    "    axes[1, i].set_title(f'Waveform: {audio_file.stem}')\n",
    "    axes[1, i].set_xlabel('Time (s)')\n",
    "    axes[1, i].set_ylabel('Amplitude')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3a31e",
   "metadata": {},
   "source": [
    "## 5. Neural Network Architecture Implementation\n",
    "\n",
    "Let's implement a simple pitch correction network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547d3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePitchCorrectionNet(nn.Module):\n",
    "    \"\"\"A simplified version of the pitch correction network for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=512, hidden_size=256, num_layers=3):\n",
    "        super(SimplePitchCorrectionNet, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Input normalization\n",
    "        self.input_norm = nn.BatchNorm1d(1)\n",
    "        \n",
    "        # Pitch embedding\n",
    "        self.pitch_embedding = nn.Linear(2, 32)  # target_pitch + correction_strength\n",
    "        \n",
    "        # Main network layers\n",
    "        layers = []\n",
    "        \n",
    "        # First layer (audio + pitch features)\n",
    "        layers.append(nn.Linear(input_size + 32, hidden_size))\n",
    "        layers.append(nn.GELU())\n",
    "        layers.append(nn.Dropout(0.1))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(0.1))\n",
    "        \n",
    "        self.main_network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Output layers\n",
    "        self.audio_output = nn.Linear(hidden_size, input_size)\n",
    "        self.confidence_output = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Residual connection weight\n",
    "        self.residual_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "    def forward(self, audio_buffer, target_pitch, correction_strength):\n",
    "        batch_size = audio_buffer.size(0)\n",
    "        \n",
    "        # Normalize audio input\n",
    "        audio_normalized = self.input_norm(audio_buffer.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Create pitch embedding\n",
    "        target_pitch_norm = torch.log(target_pitch + 1e-8) / 10.0\n",
    "        pitch_input = torch.cat([target_pitch_norm, correction_strength], dim=1)\n",
    "        pitch_features = self.pitch_embedding(pitch_input)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_input = torch.cat([audio_normalized, pitch_features], dim=1)\n",
    "        \n",
    "        # Process through network\n",
    "        features = self.main_network(combined_input)\n",
    "        \n",
    "        # Generate outputs\n",
    "        pitch_correction = self.audio_output(features)\n",
    "        confidence = self.confidence_output(features)\n",
    "        \n",
    "        # Apply residual connection\n",
    "        residual_weight = torch.sigmoid(self.residual_weight)\n",
    "        corrected_audio = (1 - residual_weight) * audio_buffer + residual_weight * pitch_correction\n",
    "        \n",
    "        # Apply correction strength\n",
    "        final_audio = (1 - correction_strength.unsqueeze(-1)) * audio_buffer + \\\n",
    "                     correction_strength.unsqueeze(-1) * corrected_audio\n",
    "        \n",
    "        return final_audio, confidence\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        return {\n",
    "            'total_parameters': total_params,\n",
    "            'input_size': self.input_size,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'model_size_mb': total_params * 4 / (1024 * 1024)\n",
    "        }\n",
    "\n",
    "# Create and test the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimplePitchCorrectionNet().to(device)\n",
    "\n",
    "print(f\"Model created on device: {device}\")\n",
    "print(f\"Model info: {model.get_model_info()}\")\n",
    "\n",
    "# Test with dummy data\n",
    "batch_size = 4\n",
    "buffer_size = 512\n",
    "\n",
    "# Create test inputs\n",
    "test_audio = torch.randn(batch_size, buffer_size).to(device)\n",
    "test_target_pitch = torch.rand(batch_size, 1).to(device) * 1000 + 100  # 100-1100 Hz\n",
    "test_correction_strength = torch.rand(batch_size, 1).to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    corrected_audio, confidence = model(test_audio, test_target_pitch, test_correction_strength)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Input audio shape: {test_audio.shape}\")\n",
    "print(f\"Output audio shape: {corrected_audio.shape}\")\n",
    "print(f\"Confidence shape: {confidence.shape}\")\n",
    "print(f\"Mean confidence: {confidence.mean().item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09396b84",
   "metadata": {},
   "source": [
    "## 6. Model Training and Validation Setup\n",
    "\n",
    "Let's set up a basic training loop with loss functions and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Multi-component loss function for pitch correction.\"\"\"\n",
    "    \n",
    "    def __init__(self, audio_weight=0.6, pitch_weight=0.3, confidence_weight=0.1):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.audio_weight = audio_weight\n",
    "        self.pitch_weight = pitch_weight\n",
    "        self.confidence_weight = confidence_weight\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, predicted_audio, target_audio, predicted_confidence, target_confidence):\n",
    "        # Audio reconstruction loss\n",
    "        audio_loss = self.mse_loss(predicted_audio, target_audio)\n",
    "        \n",
    "        # Confidence prediction loss\n",
    "        confidence_loss = self.bce_loss(predicted_confidence, target_confidence)\n",
    "        \n",
    "        # Spectral loss (simplified)\n",
    "        pred_fft = torch.fft.fft(predicted_audio, dim=-1)\n",
    "        target_fft = torch.fft.fft(target_audio, dim=-1)\n",
    "        spectral_loss = self.mse_loss(torch.abs(pred_fft), torch.abs(target_fft))\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = (self.audio_weight * audio_loss + \n",
    "                     self.pitch_weight * spectral_loss + \n",
    "                     self.confidence_weight * confidence_loss)\n",
    "        \n",
    "        return total_loss, {\n",
    "            'audio_loss': audio_loss.item(),\n",
    "            'spectral_loss': spectral_loss.item(),\n",
    "            'confidence_loss': confidence_loss.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "\n",
    "# Training utilities\n",
    "class ModelTrainer:\n",
    "    \"\"\"Simple trainer for the pitch correction model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.criterion = CombinedLoss()\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        self.history = {'train_loss': [], 'val_loss': []}\n",
    "        \n",
    "    def train_step(self, audio_batch, target_batch, pitch_batch, strength_batch, confidence_batch):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predicted_audio, predicted_confidence = self.model(\n",
    "            audio_batch, pitch_batch, strength_batch\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, loss_components = self.criterion(\n",
    "            predicted_audio, target_batch, predicted_confidence, confidence_batch\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss_components\n",
    "    \n",
    "    def validate_step(self, audio_batch, target_batch, pitch_batch, strength_batch, confidence_batch):\n",
    "        \"\"\"Single validation step.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted_audio, predicted_confidence = self.model(\n",
    "                audio_batch, pitch_batch, strength_batch\n",
    "            )\n",
    "            \n",
    "            loss, loss_components = self.criterion(\n",
    "                predicted_audio, target_batch, predicted_confidence, confidence_batch\n",
    "            )\n",
    "            \n",
    "        return loss_components\n",
    "\n",
    "# Create trainer\n",
    "trainer = ModelTrainer(model, device)\n",
    "print(f\"Trainer created with device: {device}\")\n",
    "\n",
    "# Generate synthetic training data for demonstration\n",
    "def generate_training_batch(batch_size=16, buffer_size=512):\n",
    "    \"\"\"Generate synthetic training data.\"\"\"\n",
    "    \n",
    "    # Input audio (slightly off-pitch)\n",
    "    input_audio = torch.randn(batch_size, buffer_size)\n",
    "    \n",
    "    # Target audio (corrected)\n",
    "    target_audio = input_audio + 0.1 * torch.randn(batch_size, buffer_size)\n",
    "    \n",
    "    # Target pitch (random frequencies)\n",
    "    target_pitch = torch.rand(batch_size, 1) * 1000 + 100\n",
    "    \n",
    "    # Correction strength\n",
    "    correction_strength = torch.rand(batch_size, 1)\n",
    "    \n",
    "    # Confidence (higher for cleaner audio)\n",
    "    confidence = torch.rand(batch_size, 1) * 0.5 + 0.5\n",
    "    \n",
    "    return input_audio, target_audio, target_pitch, correction_strength, confidence\n",
    "\n",
    "# Test training step\n",
    "input_audio, target_audio, target_pitch, correction_strength, confidence = generate_training_batch()\n",
    "\n",
    "# Move to device\n",
    "input_audio = input_audio.to(device)\n",
    "target_audio = target_audio.to(device)\n",
    "target_pitch = target_pitch.to(device)\n",
    "correction_strength = correction_strength.to(device)\n",
    "confidence = confidence.to(device)\n",
    "\n",
    "# Run training step\n",
    "loss_components = trainer.train_step(\n",
    "    input_audio, target_audio, target_pitch, correction_strength, confidence\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining step completed:\")\n",
    "for key, value in loss_components.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee2747",
   "metadata": {},
   "source": [
    "## 7. Model Export to ONNX Format\n",
    "\n",
    "Let's export our model to ONNX format for C++ integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f647d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_to_onnx(model, export_path, input_size=512):\n",
    "    \"\"\"Export PyTorch model to ONNX format.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy inputs\n",
    "    dummy_audio = torch.randn(1, input_size)\n",
    "    dummy_pitch = torch.tensor([[440.0]])  # A4 note\n",
    "    dummy_strength = torch.tensor([[0.5]])  # 50% correction\n",
    "    \n",
    "    # Move to same device as model\n",
    "    device = next(model.parameters()).device\n",
    "    dummy_audio = dummy_audio.to(device)\n",
    "    dummy_pitch = dummy_pitch.to(device)\n",
    "    dummy_strength = dummy_strength.to(device)\n",
    "    \n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_audio, dummy_pitch, dummy_strength),\n",
    "        export_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['audio_input', 'target_pitch', 'correction_strength'],\n",
    "        output_names=['corrected_audio', 'confidence'],\n",
    "        dynamic_axes={\n",
    "            'audio_input': {0: 'batch_size'},\n",
    "            'target_pitch': {0: 'batch_size'},\n",
    "            'correction_strength': {0: 'batch_size'},\n",
    "            'corrected_audio': {0: 'batch_size'},\n",
    "            'confidence': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Model exported to: {export_path}\")\n",
    "    \n",
    "    # Verify export\n",
    "    try:\n",
    "        import onnx\n",
    "        onnx_model = onnx.load(export_path)\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        print(\"‚úÖ ONNX model validation passed\")\n",
    "        \n",
    "        # Print model info\n",
    "        print(f\"Model inputs: {[input.name for input in onnx_model.graph.input]}\")\n",
    "        print(f\"Model outputs: {[output.name for output in onnx_model.graph.output]}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è ONNX not available for verification\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ONNX model validation failed: {e}\")\n",
    "\n",
    "# Export the model\n",
    "export_dir = Path('../models/exported')\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "export_path = export_dir / 'pitch_correction_model.onnx'\n",
    "\n",
    "export_model_to_onnx(model, export_path)\n",
    "\n",
    "# Test ONNX inference if available\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "    \n",
    "    # Create inference session\n",
    "    ort_session = ort.InferenceSession(str(export_path))\n",
    "    \n",
    "    # Prepare test input\n",
    "    test_audio_np = np.random.randn(1, 512).astype(np.float32)\n",
    "    test_pitch_np = np.array([[440.0]], dtype=np.float32)\n",
    "    test_strength_np = np.array([[0.5]], dtype=np.float32)\n",
    "    \n",
    "    # Run inference\n",
    "    ort_inputs = {\n",
    "        'audio_input': test_audio_np,\n",
    "        'target_pitch': test_pitch_np,\n",
    "        'correction_strength': test_strength_np\n",
    "    }\n",
    "    \n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "    \n",
    "    print(f\"\\n‚úÖ ONNX Runtime inference successful\")\n",
    "    print(f\"Output shapes: {[output.shape for output in ort_outputs]}\")\n",
    "    \n",
    "    # Compare with PyTorch output\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        torch_audio = torch.from_numpy(test_audio_np).to(device)\n",
    "        torch_pitch = torch.from_numpy(test_pitch_np).to(device)\n",
    "        torch_strength = torch.from_numpy(test_strength_np).to(device)\n",
    "        \n",
    "        torch_outputs = model(torch_audio, torch_pitch, torch_strength)\n",
    "        torch_audio_out = torch_outputs[0].cpu().numpy()\n",
    "        torch_conf_out = torch_outputs[1].cpu().numpy()\n",
    "    \n",
    "    # Calculate differences\n",
    "    audio_diff = np.mean(np.abs(ort_outputs[0] - torch_audio_out))\n",
    "    conf_diff = np.mean(np.abs(ort_outputs[1] - torch_conf_out))\n",
    "    \n",
    "    print(f\"PyTorch vs ONNX differences:\")\n",
    "    print(f\"  Audio output difference: {audio_diff:.6f}\")\n",
    "    print(f\"  Confidence output difference: {conf_diff:.6f}\")\n",
    "    \n",
    "    if audio_diff < 1e-5 and conf_diff < 1e-5:\n",
    "        print(\"‚úÖ PyTorch and ONNX outputs match closely\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Significant differences between PyTorch and ONNX outputs\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ONNX Runtime not available for testing\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ONNX Runtime test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06378d1",
   "metadata": {},
   "source": [
    "## 8. C++ Engine Integration Guidelines\n",
    "\n",
    "Here's how the exported model integrates with the C++ real-time engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7507cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis for C++ integration\n",
    "import time\n",
    "\n",
    "def benchmark_model_latency(model, num_iterations=100, batch_size=1, buffer_size=512):\n",
    "    \"\"\"Benchmark model inference latency.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_audio = torch.randn(batch_size, buffer_size).to(device)\n",
    "    test_pitch = torch.rand(batch_size, 1).to(device) * 1000 + 100\n",
    "    test_strength = torch.rand(batch_size, 1).to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            _ = model(test_audio, test_pitch, test_strength)\n",
    "    \n",
    "    # Benchmark\n",
    "    latencies = []\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = model(test_audio, test_pitch, test_strength)\n",
    "            \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "        end_time = time.perf_counter()\n",
    "        latency_ms = (end_time - start_time) * 1000\n",
    "        latencies.append(latency_ms)\n",
    "    \n",
    "    return {\n",
    "        'mean_latency_ms': np.mean(latencies),\n",
    "        'std_latency_ms': np.std(latencies),\n",
    "        'min_latency_ms': np.min(latencies),\n",
    "        'max_latency_ms': np.max(latencies),\n",
    "        'p95_latency_ms': np.percentile(latencies, 95)\n",
    "    }\n",
    "\n",
    "# Benchmark the model\n",
    "print(\"Benchmarking model latency...\")\n",
    "latency_results = benchmark_model_latency(model)\n",
    "\n",
    "print(\"\\nLatency Benchmark Results:\")\n",
    "for key, value in latency_results.items():\n",
    "    print(f\"  {key}: {value:.3f} ms\")\n",
    "\n",
    "# Check real-time requirements\n",
    "buffer_duration_ms = 512 / 44100 * 1000  # ~11.6 ms for 512 samples at 44.1kHz\n",
    "target_latency_ms = 5.0  # Target from ML integration guide\n",
    "\n",
    "print(f\"\\nReal-time Analysis:\")\n",
    "print(f\"Buffer duration: {buffer_duration_ms:.1f} ms\")\n",
    "print(f\"Target latency: {target_latency_ms} ms\")\n",
    "print(f\"Mean inference latency: {latency_results['mean_latency_ms']:.1f} ms\")\n",
    "\n",
    "if latency_results['mean_latency_ms'] <= target_latency_ms:\n",
    "    print(\"‚úÖ Model meets real-time latency requirements\")\n",
    "else:\n",
    "    print(\"‚ùå Model exceeds real-time latency requirements\")\n",
    "    print(\"   Consider model optimization or hardware acceleration\")\n",
    "\n",
    "# Model size analysis\n",
    "model_info = model.get_model_info()\n",
    "print(f\"\\nModel Size Analysis:\")\n",
    "print(f\"Parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\"Model size: {model_info['model_size_mb']:.1f} MB\")\n",
    "\n",
    "# Memory footprint estimate\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    memory_before = torch.cuda.memory_allocated() / 1024**2\n",
    "    \n",
    "    # Load model\n",
    "    test_audio = torch.randn(1, 512).cuda()\n",
    "    test_pitch = torch.rand(1, 1).cuda() * 1000 + 100\n",
    "    test_strength = torch.rand(1, 1).cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(test_audio, test_pitch, test_strength)\n",
    "    \n",
    "    memory_after = torch.cuda.memory_allocated() / 1024**2\n",
    "    memory_usage = memory_after - memory_before\n",
    "    \n",
    "    print(f\"GPU memory usage: {memory_usage:.1f} MB\")\n",
    "    \n",
    "    # Check memory requirements\n",
    "    target_memory_mb = 100  # Target from ML integration guide\n",
    "    if memory_usage <= target_memory_mb:\n",
    "        print(\"‚úÖ Model meets memory requirements\")\n",
    "    else:\n",
    "        print(\"‚ùå Model exceeds memory requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb51f9d",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook has demonstrated the complete pipeline for developing AutoTune ML models:\n",
    "\n",
    "### What We've Accomplished:\n",
    "1. **Environment Setup** - Validated dependencies and configuration\n",
    "2. **Audio Processing** - Loaded and analyzed audio data\n",
    "3. **Pitch Detection** - Compared CREPE and Librosa algorithms\n",
    "4. **Dataset Creation** - Built sample dataset and analysis pipeline\n",
    "5. **Neural Networks** - Implemented pitch correction architecture\n",
    "6. **Training Setup** - Created training loops and loss functions\n",
    "7. **Model Export** - Exported to ONNX for C++ integration\n",
    "8. **Performance Analysis** - Benchmarked for real-time requirements\n",
    "\n",
    "### Next Steps:\n",
    "1. **Collect Real Data** - Gather vocal and instrumental recordings\n",
    "2. **Train Full Models** - Use the training scripts in `scripts/`\n",
    "3. **Evaluate Performance** - Test on real audio samples\n",
    "4. **Optimize for Production** - Reduce latency and memory usage\n",
    "5. **Integrate with C++** - Deploy in the real-time AutoTune engine\n",
    "\n",
    "### For YouTube Content (Sergie Code):\n",
    "- **Part 1**: Environment setup and audio basics\n",
    "- **Part 2**: Pitch detection and analysis\n",
    "- **Part 3**: Neural network architecture\n",
    "- **Part 4**: Training and evaluation\n",
    "- **Part 5**: Model export and C++ integration\n",
    "\n",
    "Happy coding! üéµ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
